{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg4ZUdVfEXYohLtXfQGSLh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranamaddy/Introduction-to-Coding-in-Pandas-Using-Python/blob/main/Lesson_4_Data_Analysis_and_Transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 4: Data Analysis and Transformation\n",
        "- Aggregating and summarizing data using Pandas\n",
        "- Grouping and grouping operations\n",
        "- Applying functions to data (apply, map, applymap)\n",
        "- Data transformation techniques (merging, joining, reshaping)\n",
        "- Handling duplicates and outliers\n",
        "- Introduction to descriptive statistics"
      ],
      "metadata": {
        "id": "8fo1mPkmIEkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to Lesson 4: Data Analysis and Transformation. In this lesson, we will explore various techniques for analyzing and transforming data using Pandas. Data analysis involves examining, summarizing, and drawing insights from data, while data transformation involves manipulating and reshaping the data to make it more suitable for analysis.\n",
        "\n",
        "**Throughout this lesson, we will cover the following topics:**\n",
        "\n",
        "- Descriptive Statistics: We will learn how to calculate and interpret descriptive statistics, such as measures of central tendency (mean, median, mode) and measures of variability (standard deviation, range). Descriptive statistics help us understand the characteristics and distribution of the data.\n",
        "\n",
        "- Aggregation and Grouping: We will explore how to group data based on specific criteria and perform aggregations, such as calculating sums, averages, and counts within each group. Grouping data allows us to analyze subsets of the data and derive insights at different levels of granularity.\n",
        "\n",
        "- Data Cleaning and Preprocessing: We will discuss techniques for handling missing data, dealing with duplicate values, and addressing outliers. Cleaning and preprocessing the data is essential to ensure the accuracy and integrity of the analysis.\n",
        "\n",
        "- Data Transformation: We will cover various data transformation techniques, including reshaping data, pivoting tables, and melting data. These transformations help us restructure the data to make it more suitable for specific analytical tasks.\n",
        "\n",
        "- Merging and Joining Data: We will learn how to combine multiple datasets based on common columns or indices. Merging and joining data allows us to integrate information from different sources and perform more comprehensive analysis.\n",
        "\n",
        "- Handling Categorical Data: We will explore methods for handling categorical variables, including encoding categorical data, creating dummy variables, and performing one-hot encoding. Categorical variables require special treatment to make them usable in data analysis.\n",
        "\n",
        "By the end of this lesson, you will have a solid foundation in data analysis and transformation using Pandas. You will be able to apply various techniques to gain insights from data, perform data cleaning and preprocessing, transform data structures, merge datasets, and handle categorical variables effectively.\n",
        "\n",
        "Let's start with the first topic: Descriptive Statistics."
      ],
      "metadata": {
        "id": "BY2o5LSFIUmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregating and summarizing data using Pandas\n",
        "\n",
        "Let's explore how to aggregate and summarize data using Pandas, using the given dataset \"data.csv\" with attributes like Student ID, Class, Study hrs, Sleeping hrs, Social Media usage hrs, Mobile Games hrs, and Percentage.\n",
        "\n",
        "Aggregating data involves grouping the dataset based on certain criteria and performing calculations on the grouped data. Pandas provides convenient methods to perform aggregation operations.\n",
        "\n",
        "Here's an example of how to aggregate and summarize the data"
      ],
      "metadata": {
        "id": "3Qmfz1q0I4-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Group the data by 'Class'\n",
        "grouped_data = df.groupby('Class')\n",
        "\n",
        "# Calculate the average study hours for each class\n",
        "average_study_hours = grouped_data['Study hrs'].mean()\n",
        "\n",
        "# Calculate the maximum percentage for each class\n",
        "maximum_percentage = grouped_data['Percantege'].max()\n",
        "\n",
        "# Calculate the total sleeping hours for each class\n",
        "total_sleeping_hours = grouped_data['Sleeping hrs'].sum()\n",
        "\n",
        "# Print the results\n",
        "print(\"Average study hours by class:\")\n",
        "print(average_study_hours)\n",
        "print(\"\\nMaximum percentage by class:\")\n",
        "print(maximum_percentage)\n",
        "print(\"\\nTotal sleeping hours by class:\")\n",
        "print(total_sleeping_hours)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frEoeWDAI-Yu",
        "outputId": "25a71c3b-2a2a-4a27-9100-d7971f5702ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average study hours by class:\n",
            "Class\n",
            "10    3.666667\n",
            "11    2.000000\n",
            "12    5.500000\n",
            "Name: Study hrs, dtype: float64\n",
            "\n",
            "Maximum percentage by class:\n",
            "Class\n",
            "10    80\n",
            "11    96\n",
            "12    90\n",
            "Name: Percantege, dtype: int64\n",
            "\n",
            "Total sleeping hours by class:\n",
            "Class\n",
            "10    25\n",
            "11    24\n",
            "12    29\n",
            "Name: Sleeping hrs, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the dataset using pd.**read_csv**() and assign it to the variable df. Next, we group the data by the '**Class**' column using groupby('**Class**'), creating a GroupBy object named grouped_data.\n",
        "\n",
        "We then apply various aggregation functions to the grouped data. For example, we calculate the average study hours for each class by accessing the 'Study hrs' column of the grouped_data object and using the **mean**() function. Similarly, we calculate the maximum percentage and the total sleeping hours for each class using the **max**() and **sum**() functions, respectively.\n",
        "\n",
        "Finally, we print the results to display the aggregated and summarized data.\n",
        "\n",
        "Feel free to explore other aggregation functions such as **min**(), **count**(), **std**(), etc., to derive different insights from the data. Additionally, you can aggregate the data based on multiple columns by passing a list of column names to the **groupby**() function.\n",
        "\n",
        "Aggregating and summarizing data helps us understand the characteristics and trends within different groups, providing valuable insights for analysis and decision-making."
      ],
      "metadata": {
        "id": "H_g1rqCJJQJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grouping and grouping operations\n",
        "\n",
        "Let's delve into grouping and grouping operations in Pandas, explaining it as if you were a beginner student.\n",
        "\n",
        "Grouping is a fundamental concept in data analysis that allows us to divide the data into subsets based on specific criteria and perform calculations or operations on each group. Pandas provides powerful tools for grouping data and conducting operations on the grouped data.\n",
        "\n",
        "**To group data in Pandas, we typically follow these steps:**\n",
        "\n",
        "\n",
        "- Load the dataset: Begin by loading your dataset into a Pandas DataFrame using functions like pd.read_csv() for CSV files or pd.read_excel() for Excel files.\n",
        "\n",
        "- Identify the grouping column(s): Determine the column(s) in your dataset that you want to use for grouping. These columns should contain categorical or qualitative data that define the groups.\n",
        "\n",
        "- Group the data: Use the groupby() function on your DataFrame, passing the column(s) you identified as the grouping criterion. This creates a GroupBy object, which is a powerful tool for performing operations on the grouped data.\n",
        "\n",
        "- Perform operations on the grouped data: Once you have the GroupBy object, you can apply various operations to the grouped data. Some commonly used operations include calculating summary statistics (e.g., mean, sum, count) using functions like mean(), sum(), or count(). You can also apply custom functions or methods to perform more complex calculations or transformations.\n",
        "\n",
        "Here's an example to illustrate these steps:"
      ],
      "metadata": {
        "id": "xwXefwU3JtsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Group the data by 'Class'\n",
        "grouped_data = df.groupby('Class')\n",
        "\n",
        "# Calculate the average study hours for each class\n",
        "average_study_hours = grouped_data['Study hrs'].mean()\n",
        "\n",
        "# Calculate the maximum percentage for each class\n",
        "maximum_percentage = grouped_data['Percantege'].max()\n",
        "\n",
        "# Calculate the total sleeping hours for each class\n",
        "total_sleeping_hours = grouped_data['Sleeping hrs'].sum()\n"
      ],
      "metadata": {
        "id": "eHQbskAqJ8Zg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the dataset into a DataFrame named df. Then, we group the data by the 'Class' column using groupby('**Class**'), creating a GroupBy object called grouped_data.\n",
        "\n",
        "Once we have the grouped data, we can apply various operations. For instance, we calculate the average study hours for each class by accessing the 'Study hrs' column of the grouped_data object and using the **mean**() function. Similarly, we calculate the maximum percentage and the total sleeping hours for each class using the **max**() and **sum**() functions, respectively.\n",
        "\n",
        "Grouping operations allow us to analyze data at different levels of granularity, compare statistics across groups, and derive insights specific to each group. It helps in understanding patterns, trends, and distributions within different subsets of the data.\n",
        "\n",
        "Remember that grouping is just the starting point, and you can combine grouping with other data manipulation and visualization techniques to gain deeper insights from your data."
      ],
      "metadata": {
        "id": "70N4OHcPKLif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying functions to data (apply, map, applymap)\n",
        "\n",
        " Let's explore the functions apply(), map(), and applymap() in Pandas, explaining them in a beginner-friendly manner.\n",
        "\n",
        "1. apply(): The apply() function in Pandas allows you to apply a function to each element, row, or column of a DataFrame. It is a versatile function that enables you to perform custom operations on your data.\n",
        "Applying a function to each element: You can use apply() on a Series or DataFrame to apply a function to each individual element. This is useful when you need to perform element-wise calculations or transformations.\n",
        "\n",
        "2. Applying a function to each row or column: By specifying the axis parameter, you can apply a function to each row (axis=0) or column (axis=1) of a DataFrame. This allows you to perform row-wise or column-wise operations.\n",
        "\n",
        "2. map(): The map() function in Pandas is primarily used to substitute values in a Series or DataFrame based on a defined mapping or dictionary.\n",
        "3. Mapping values in a Series: You can use map() on a Series to replace specific values with new values based on a defined mapping. This is useful when you want to perform value substitutions or convert categorical data into numerical representations.\n",
        "3. applymap(): The applymap() function is similar to map(), but it operates element-wise on an entire DataFrame, rather than a single Series.\n",
        "- Applying a function element-wise to a DataFrame: applymap() allows you to apply a function to each element of a DataFrame, resulting in a new DataFrame with the transformed values. This function is useful when you need to perform element-wise operations across the entire DataFrame.\n",
        "\n",
        "Here's an example that demonstrates these functions:"
      ],
      "metadata": {
        "id": "pQT1bIRfKVTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from data.csv\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Example of apply()\n",
        "# Applying a function to each element\n",
        "df['Study hrs squared'] = df['Study hrs'].apply(lambda x: x ** 2)\n",
        "\n",
        "# Applying a function to each row or column\n",
        "df['Total hrs'] = df[['Study hrs', 'Sleeping hrs']].apply(sum, axis=1)\n",
        "\n",
        "# Example of map()\n",
        "# Mapping values in a Series\n",
        "grade_mapping = {'A': 'Excellent', 'B': 'Good', 'C': 'Average'}\n",
        "df['Grade'] = df['Class'].map(grade_mapping)\n",
        "\n",
        "# Example of applymap()\n",
        "# Applying a function element-wise to a DataFrame\n",
        "df_numeric = df.select_dtypes(include='number')\n",
        "#df_numeric_avg = df_numeric.applymap(lambda x: x.mean())\n",
        "\n",
        "# Print the modified DataFrame\n",
        "print(df)\n",
        "#print(df_numeric_avg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvZgEYivLTJN",
        "outputId": "6c212ba9-8814-4bf1-be2c-8e8520577d04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Student ID  Class  Study hrs  Sleeping hrs  Social Media usage hrs  \\\n",
            "0        1001     10        2.0             9                     3.0   \n",
            "1        1002     10        6.0             8                     2.0   \n",
            "2        1003     10        3.0             8                     2.0   \n",
            "3        1004     11        0.0            10                     1.0   \n",
            "4        1005     11        4.0             7                     NaN   \n",
            "5        1006     11        NaN             7                     0.0   \n",
            "6        1007     12        4.0             6                     0.0   \n",
            "7        1008     12       10.0             6                     2.0   \n",
            "8        1009     12        2.0             8                     2.0   \n",
            "9        1010     12        6.0             9                     1.0   \n",
            "\n",
            "   Mobile Games hrs  Percantege  Study hrs squared  Total hrs Grade  \n",
            "0               5.0          50                4.0       11.0   NaN  \n",
            "1               0.0          80               36.0       14.0   NaN  \n",
            "2               NaN          60                9.0       11.0   NaN  \n",
            "3               5.0          45                0.0       10.0   NaN  \n",
            "4               0.0          75               16.0       11.0   NaN  \n",
            "5               0.0          96                NaN        NaN   NaN  \n",
            "6               0.0          80               16.0       10.0   NaN  \n",
            "7               0.0          90              100.0       16.0   NaN  \n",
            "8               4.0          60                4.0       10.0   NaN  \n",
            "9               0.0          85               36.0       15.0   NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, we first import the Pandas library and load the dataset \"data.csv\" using the **read_csv**() function, storing it in the DataFrame df.\n",
        "\n",
        "We then demonstrate the use of **apply**(), where we apply a lambda function to calculate the square of each \"**Study** **hrs**\" value and assign it to a new column called \"**Study hrs squared**\". We also apply the **sum**() function to calculate the total hours spent studying and sleeping for each student, storing the result in a new column called \"Total hrs\".\n",
        "\n",
        "Next, we showcase the **map**() function by mapping the \"Class\" values to corresponding \"Grade\" values using a mapping dictionary. The mapped values are stored in a new column called \"Grade\".\n",
        "\n",
        "Finally, we use **applymap**() to apply a lambda function element-wise to the numerical columns of the DataFrame. We select the numerical columns using **select_dtypes**() and then calculate the average of each column, storing the results in the DataFrame **df_numeric_avg**.\n",
        "\n",
        "These examples demonstrate how to use apply(), map(), and applymap() to perform various operations on the dataset, allowing you to apply functions to the data in different ways based on your analysis requirements."
      ],
      "metadata": {
        "id": "XiB_sLm9L27W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data transformation techniques (merging, joining, reshaping)\n",
        "\n",
        " Let's explain data transformation techniques like merging, joining, and reshaping in a beginner-friendly manner\n",
        "\n",
        "1. **Merging DataFrames:**\n",
        "\n",
        " - Merging combines two or more DataFrames based on a common column or index. It is useful when you want to combine datasets that share a common key.\n",
        " - There are different types of merges, such as inner merge, left merge, right merge, and outer merge, depending on which records you want to include in the resulting DataFrame.\n",
        " - Example: Merging two DataFrames based on a common column 'Student ID':\n",
        " \n"
      ],
      "metadata": {
        "id": "7bCiwPY2MiAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uaSIPQw0IBIU"
      },
      "outputs": [],
      "source": [
        "df1=df2=df\n",
        "merged_df = df1.merge(df2, on='Student ID', how='inner')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Joining DataFrames:**\n",
        "\n",
        " - Joining is similar to merging but is based on the index rather than a specific column. It combines DataFrames based on their index values.\n",
        " - Example: Joining two DataFrames based on the index:"
      ],
      "metadata": {
        "id": "0oxzCkKRNEXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = df1.join(df2, lsuffix='_left', rsuffix='_right')\n"
      ],
      "metadata": {
        "id": "XTApYstCNKWm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Reshaping DataFrames:**\n",
        "\n",
        " - Reshaping involves transforming the structure of the DataFrame to make it more suitable for analysis or visualization.\n",
        " - Common reshaping techniques include pivoting, melting, and stacking/unstacking.\n",
        " - Pivoting: Reshapes the DataFrame by converting unique values from one column into new columns."
      ],
      "metadata": {
        "id": "LYgFXjLbNNI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivoted_df = df.pivot(index='Student ID', columns='Class', values='Percantege')\n"
      ],
      "metadata": {
        "id": "uapR_0XPNTx7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Melting: Unpivots a DataFrame, converting multiple columns into a single column and creating corresponding value columns."
      ],
      "metadata": {
        "id": "bEcP3fS6Nir0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "melted_df = pd.melt(df, id_vars='Student ID', value_vars=['Study hrs', 'Sleeping hrs'], var_name='Activity', value_name='Hours')\n"
      ],
      "metadata": {
        "id": "wlpfZBnXNltW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Stacking/Unstacking: Stacking combines multiple columns into a single column, while unstacking splits a column into multiple columns."
      ],
      "metadata": {
        "id": "71YMradpNps_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_df = df.stack()\n",
        "unstacked_df = df.unstack()\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3oJo7yHvNsJx",
        "outputId": "2c331095-2548-4eca-cb36-6317b3c56ea5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Student ID  Class  Study hrs  Sleeping hrs  Social Media usage hrs  \\\n",
              "0        1001     10        2.0             9                     3.0   \n",
              "1        1002     10        6.0             8                     2.0   \n",
              "2        1003     10        3.0             8                     2.0   \n",
              "3        1004     11        0.0            10                     1.0   \n",
              "4        1005     11        4.0             7                     NaN   \n",
              "5        1006     11        NaN             7                     0.0   \n",
              "6        1007     12        4.0             6                     0.0   \n",
              "7        1008     12       10.0             6                     2.0   \n",
              "8        1009     12        2.0             8                     2.0   \n",
              "9        1010     12        6.0             9                     1.0   \n",
              "\n",
              "   Mobile Games hrs  Percantege  Study hrs squared  Total hrs Grade  \n",
              "0               5.0          50                4.0       11.0   NaN  \n",
              "1               0.0          80               36.0       14.0   NaN  \n",
              "2               NaN          60                9.0       11.0   NaN  \n",
              "3               5.0          45                0.0       10.0   NaN  \n",
              "4               0.0          75               16.0       11.0   NaN  \n",
              "5               0.0          96                NaN        NaN   NaN  \n",
              "6               0.0          80               16.0       10.0   NaN  \n",
              "7               0.0          90              100.0       16.0   NaN  \n",
              "8               4.0          60                4.0       10.0   NaN  \n",
              "9               0.0          85               36.0       15.0   NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c15755aa-3764-44fc-9df5-211d1222509d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Student ID</th>\n",
              "      <th>Class</th>\n",
              "      <th>Study hrs</th>\n",
              "      <th>Sleeping hrs</th>\n",
              "      <th>Social Media usage hrs</th>\n",
              "      <th>Mobile Games hrs</th>\n",
              "      <th>Percantege</th>\n",
              "      <th>Study hrs squared</th>\n",
              "      <th>Total hrs</th>\n",
              "      <th>Grade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001</td>\n",
              "      <td>10</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50</td>\n",
              "      <td>4.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002</td>\n",
              "      <td>10</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80</td>\n",
              "      <td>36.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1003</td>\n",
              "      <td>10</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>60</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004</td>\n",
              "      <td>11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1005</td>\n",
              "      <td>11</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75</td>\n",
              "      <td>16.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1006</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>96</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1007</td>\n",
              "      <td>12</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80</td>\n",
              "      <td>16.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1008</td>\n",
              "      <td>12</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>90</td>\n",
              "      <td>100.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1009</td>\n",
              "      <td>12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>60</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1010</td>\n",
              "      <td>12</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85</td>\n",
              "      <td>36.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c15755aa-3764-44fc-9df5-211d1222509d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c15755aa-3764-44fc-9df5-211d1222509d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c15755aa-3764-44fc-9df5-211d1222509d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These data transformation techniques allow you to reshape and combine datasets to derive meaningful insights. By using merging and joining, you can bring together related information from multiple datasets. Reshaping techniques help you restructure the data to facilitate analysis or create summary statistics.\n",
        "\n",
        "Remember to replace **df1, df2,** and other variable names with the actual names of your DataFrames when applying these techniques to your specific datasets."
      ],
      "metadata": {
        "id": "XbOk15dIN6Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling duplicates and outliers\n",
        "\n",
        "Let's explain how to handle duplicates and outliers in a beginner-friendly manner\n",
        "\n",
        "1. **Handling Duplicates:**\n",
        "\n",
        " - Duplicates refer to rows in a DataFrame that have identical values across all or selected columns.\n",
        " - Identifying duplicates: You can use the duplicated() function to check for duplicate rows in a DataFrame. The function returns a Boolean Series indicating which rows are duplicates."
      ],
      "metadata": {
        "id": "m5mTKd-nOA-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = df.duplicated()\n"
      ],
      "metadata": {
        "id": "RwT1tGMVOQaj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Removing duplicates: To remove duplicate rows from a DataFrame, you can use the drop_duplicates() function. By default, it keeps the first occurrence of each duplicate row and removes the subsequent duplicates."
      ],
      "metadata": {
        "id": "Ezc_CD_EOUxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_duplicates = df.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "V0p-ZC0-OXYD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Handling Outliers:**\n",
        "\n",
        " - Outliers are extreme values in a dataset that significantly deviate from the majority of the data points. Outliers can negatively impact data analysis and modeling.\n",
        " - Identifying outliers: One common approach is to use statistical measures like the z-score or interquartile range (IQR) to detect outliers. Points that fall outside a certain threshold can be considered outliers.\n",
        " - Removing outliers: Depending on the situation, outliers can be removed or treated differently. One approach is to use filtering based on z-scores or IQR to exclude outliers from the analysis."
      ],
      "metadata": {
        "id": "J1YOkLnUObtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Removing outliers based on z-score\n",
        "z_scores = (df['Study hrs'] - df['Mobile Games hrs'].mean()) / df['Sleeping hrs'].std()\n",
        "df_no_outliers = df[z_scores < 3]  # Exclude data points with z-score > 3\n",
        "z_scores\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlkO2nFOOiUD",
        "outputId": "c7413b34-13de-474d-cc01-28fa141c8b2c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.337580\n",
              "1    3.375798\n",
              "2    1.097134\n",
              "3   -1.181529\n",
              "4    1.856689\n",
              "5         NaN\n",
              "6    1.856689\n",
              "7    6.414016\n",
              "8    0.337580\n",
              "9    3.375798\n",
              "Name: Study hrs, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to note that the approach for handling duplicates and outliers may vary depending on the specific dataset and analysis goals. Consider the context and the impact of removing or treating duplicates/outliers on your analysis results.\n",
        "\n",
        "Remember to replace df and 'Column' with the actual variable name of your DataFrame and the column name you want to analyze for duplicates/outliers.\n",
        "\n",
        "Handling duplicates and outliers helps ensure data quality and reliability in your analysis. By removing duplicates, you avoid counting redundant data points, and by identifying and handling outliers, you ensure more accurate and representative analysis results."
      ],
      "metadata": {
        "id": "e8eCAV6MPGjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to descriptive statistics\n",
        "\n",
        "Descriptive statistics provides a way to summarize and describe the main characteristics of a dataset. It helps us understand the basic features of the data and gain insights into its distribution, central tendency, variability, and other important properties. Here's an introduction to descriptive statistics:\n",
        "\n",
        "1. **Measures of Central Tendency:**\n",
        "\n",
        " - **Central** tendency measures help us understand the typical or central value around which the data tends to cluster.\n",
        " - **Mean**: The arithmetic average of the data points. It is calculated by summing all the values and dividing by the number of observations.\n",
        " - **Median**: The middle value in a dataset when it is sorted in ascending or descending order. It divides the data into two equal halves.\n",
        " - **Mode**: The value that occurs most frequently in the dataset.\n",
        "\n",
        "2. **Measures of Variability:**\n",
        "\n",
        " - **Variability** measures quantify the spread or dispersion of the data points and provide insights into how much the data values deviate from the central tendency.\n",
        " - **Range**: The difference between the maximum and minimum values in the dataset.\n",
        " - **Variance**: The average of the squared differences between each data point and the mean. It indicates the average variability of the data.\n",
        " - **Standard Deviation**: The square root of the variance. It measures the dispersion of the data around the mean."
      ],
      "metadata": {
        "id": "qxdTdYCIPQjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Distribution Characteristics:**\n",
        "\n",
        " - **Skewness**: Measures the asymmetry of the data distribution. A positive skew indicates a longer tail on the right side, while a negative skew indicates a longer tail on the left side.\n",
        " - **Kurtosis**: Measures the shape of the distribution's tails. It indicates whether the distribution is more peaked or flatter than a normal distribution.\n",
        "\n",
        "4. **Percentiles**:\n",
        "\n",
        " - **Percentiles** divide the data into equal parts based on their rank or position.\n",
        " - **Median** (50th percentile) divides the data into two equal halves.\n",
        " - **Quartiles** (25th and 75th percentiles) divide the data into four equal parts, providing insights into the lower and upper halves of the dataset\n",
        "\n",
        " Descriptive statistics provides a concise summary of the dataset, allowing us to understand its key characteristics without diving into detailed analysis. These measures serve as a foundation for further analysis and decision-making in various fields, including research, business, and data science."
      ],
      "metadata": {
        "id": "DNk5QuONP0Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's consider your dataset** with the attributes: Student ID, Class, Study hrs, Sleeping hrs, Social Media usage hrs, Mobile Games hrs, Percentage. Here's an example of how you can compute some descriptive statistics using Pandas:"
      ],
      "metadata": {
        "id": "89tJS0CpQTbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the dataset into a DataFrame\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Compute the measures of central tendency\n",
        "mean_percentage = df['Percantege'].mean()\n",
        "median_percentage = df['Percantege'].median()\n",
        "mode_class = df['Class'].mode()[0]\n",
        "\n",
        "# Compute the measures of variability\n",
        "data_range = df['Percantege'].max() - df['Percantege'].min()\n",
        "variance_percentage = df['Percantege'].var()\n",
        "std_deviation_percentage = df['Percantege'].std()\n",
        "\n",
        "# Compute skewness and kurtosis\n",
        "skewness = df['Percantege'].skew()\n",
        "kurtosis = df['Percantege'].kurt()\n",
        "\n",
        "# Compute percentiles\n",
        "percentile_25 = df['Percantege'].quantile(0.25)\n",
        "percentile_75 = df['Percantege'].quantile(0.75)\n",
        "\n",
        "# Print the computed statistics\n",
        "print(f\"Mean Percentage: {mean_percentage}\")\n",
        "print(f\"Median Percentage: {median_percentage}\")\n",
        "print(f\"Mode Class: {mode_class}\")\n",
        "print(f\"Range of Percentage: {data_range}\")\n",
        "print(f\"Variance of Percentage: {variance_percentage}\")\n",
        "print(f\"Standard Deviation of Percentage: {std_deviation_percentage}\")\n",
        "print(f\"Skewness of Percentage: {skewness}\")\n",
        "print(f\"Kurtosis of Percentage: {kurtosis}\")\n",
        "print(f\"25th Percentile: {percentile_25}\")\n",
        "print(f\"75th Percentile: {percentile_75}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQF5tDM6QX44",
        "outputId": "ef02e58d-7cf5-45a5-d8d8-d57bb355bb54"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Percentage: 72.1\n",
            "Median Percentage: 77.5\n",
            "Mode Class: 12\n",
            "Range of Percentage: 51\n",
            "Variance of Percentage: 300.76666666666665\n",
            "Standard Deviation of Percentage: 17.342625714310582\n",
            "Skewness of Percentage: -0.31140540519941884\n",
            "Kurtosis of Percentage: -1.2383174436662463\n",
            "25th Percentile: 60.0\n",
            "75th Percentile: 83.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we read the dataset from the 'data.csv' file into a DataFrame. Then, we compute various descriptive statistics using Pandas functions. Finally, we print the computed statistics.\n",
        "\n",
        "Please ensure that you have the 'data.csv' file in the correct location or update the file path accordingly in the code.\n"
      ],
      "metadata": {
        "id": "_na0Z5IyQvrg"
      }
    }
  ]
}